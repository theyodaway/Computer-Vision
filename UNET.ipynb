{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UKy7MdhyHu-b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def doub_conv(i_ch,o_ch):\n",
        "  conv = nn.Sequential(\n",
        "      nn.Conv2d(i_ch,o_ch,kernel_size=3),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(o_ch,o_ch,kernel_size=3),\n",
        "      nn.ReLU(inplace=True)\n",
        "  )\n",
        "  return conv\n",
        "\n",
        "def crop_img(tensor,tar_tensor):\n",
        "  target_size=tar_tensor.size()[2]\n",
        "  tensor_size=tensor.size()[2]\n",
        "  siz=tensor_size-target_size\n",
        "  siz=siz//2\n",
        "  return tensor[ :, :, siz:tensor_size-siz , siz:tensor_size-siz]"
      ],
      "metadata": {
        "id": "RzMnWgEsW3N1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(UNet,self).__init__()\n",
        "    self.mx_pl_22= nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "    self.dwn_conv_1=doub_conv(1,64)\n",
        "    self.dwn_conv_2=doub_conv(64,128)\n",
        "    self.dwn_conv_3=doub_conv(128,256)\n",
        "    self.dwn_conv_4=doub_conv(256,512)\n",
        "    self.dwn_conv_5=doub_conv(512,1024)\n",
        "    self.up_trans_1=nn.ConvTranspose2d(in_channels=1024, \n",
        "                                      out_channels=512, \n",
        "                                      kernel_size=2, \n",
        "                                      stride=2)\n",
        "    \n",
        "    self.up_conv_1=doub_conv(1024,512)\n",
        "\n",
        "    self.up_trans_2=nn.ConvTranspose2d(in_channels=512, \n",
        "                                      out_channels=256, \n",
        "                                      kernel_size=2, \n",
        "                                      stride=2)\n",
        "    \n",
        "    self.up_conv_2=doub_conv(512,256)\n",
        "\n",
        "    self.up_trans_3=nn.ConvTranspose2d(in_channels=256, \n",
        "                                      out_channels=128, \n",
        "                                      kernel_size=2, \n",
        "                                      stride=2)\n",
        "    \n",
        "    self.up_conv_3=doub_conv(256,128)\n",
        "\n",
        "\n",
        "    self.up_trans_4=nn.ConvTranspose2d(in_channels=128, \n",
        "                                      out_channels=64, \n",
        "                                      kernel_size=2, \n",
        "                                      stride=2\n",
        "                                      )\n",
        "    \n",
        "    self.up_conv_4=doub_conv(128,64)        \n",
        "\n",
        "    self.out = nn.Conv2d(in_channels=64,out_channels=2,kernel_size=1)\n",
        "    \n",
        "\n",
        "  def forward(self,image):\n",
        "    #encoder\n",
        "    x1=self.dwn_conv_1(image)\n",
        "    print(x1.size())\n",
        "    x2=self.mx_pl_22(x1)\n",
        "    x3=self.dwn_conv_2(x2)\n",
        "    x4=self.mx_pl_22(x3)\n",
        "    print(x4.size())\n",
        "    x5=self.dwn_conv_3(x4)\n",
        "    x6=self.mx_pl_22(x5)\n",
        "    x7=self.dwn_conv_4(x6)\n",
        "    x8=self.mx_pl_22(x7)\n",
        "    x9=self.dwn_conv_5(x8)\n",
        "    print(x9.size())\n",
        "\n",
        "\n",
        "    #decoder\n",
        "    x=self.up_trans_1(x9)\n",
        "    y=crop_img(x7,x)\n",
        "    x_1=self.up_conv_1(torch.cat([x,y],1))\n",
        "\n",
        "    x=self.up_trans_2(x_1)\n",
        "    y=crop_img(x5,x)\n",
        "    x_2=self.up_conv_2(torch.cat([x,y],1))\n",
        "\n",
        "    x=self.up_trans_3(x_2)\n",
        "    y=crop_img(x3,x)\n",
        "    x_3=self.up_conv_3(torch.cat([x,y],1))\n",
        "\n",
        "    x=self.up_trans_4(x_3)\n",
        "    y=crop_img(x1,x)\n",
        "    x4=self.up_conv_4(torch.cat([x,y],1))\n",
        "    out=self.out(x4)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "ES4uW9HhSSHb"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  image=torch.rand((1,1,572,572))\n",
        "  model=UNet()\n",
        "  print(model(image))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5rxCynLp0Uf",
        "outputId": "ef4e2921-2fe5-4190-d1e6-1f938348c6e4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 568, 568])\n",
            "torch.Size([1, 128, 140, 140])\n",
            "torch.Size([1, 1024, 28, 28])\n",
            "out torch.Size([1, 2, 388, 388])\n",
            "tensor([[[[ 0.0685,  0.0632,  0.0680,  ...,  0.0684,  0.0640,  0.0624],\n",
            "          [ 0.0693,  0.0669,  0.0651,  ...,  0.0666,  0.0640,  0.0653],\n",
            "          [ 0.0695,  0.0697,  0.0652,  ...,  0.0658,  0.0686,  0.0702],\n",
            "          ...,\n",
            "          [ 0.0709,  0.0665,  0.0688,  ...,  0.0645,  0.0635,  0.0690],\n",
            "          [ 0.0667,  0.0653,  0.0651,  ...,  0.0663,  0.0646,  0.0673],\n",
            "          [ 0.0633,  0.0669,  0.0659,  ...,  0.0659,  0.0719,  0.0639]],\n",
            "\n",
            "         [[-0.0695, -0.0718, -0.0698,  ..., -0.0669, -0.0685, -0.0701],\n",
            "          [-0.0766, -0.0710, -0.0707,  ..., -0.0679, -0.0693, -0.0718],\n",
            "          [-0.0696, -0.0707, -0.0732,  ..., -0.0718, -0.0755, -0.0718],\n",
            "          ...,\n",
            "          [-0.0690, -0.0702, -0.0741,  ..., -0.0701, -0.0710, -0.0708],\n",
            "          [-0.0668, -0.0748, -0.0695,  ..., -0.0730, -0.0711, -0.0697],\n",
            "          [-0.0701, -0.0700, -0.0660,  ..., -0.0717, -0.0757, -0.0698]]]],\n",
            "       grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5hx89xy2sKJw"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}